# Implied Volatility Forecasting Pipeline

## Overview

This project presents an advanced machine learning pipeline for forecasting 10-second-ahead implied volatility (IV) for the ETH cryptocurrency, based on high-frequency order book data. The solution is developed for a time-series forecasting competition and is optimized for performance and reproducibility within a Kaggle Notebook environment.

The core of the solution is a robust **ensemble model** composed of LightGBM and CatBoost regressors. It incorporates a sophisticated feature engineering process and a rigorous validation strategy to achieve high predictive accuracy. The primary performance metric is the **Pearson Correlation Score**.

## Pipeline Steps

The pipeline is structured into several key steps to ensure a reliable and performant solution:

1.  **Data Loading:** The script automatically attempts to load the ETH training and testing datasets from the specified Kaggle paths.
2.  **Feature Engineering:** A comprehensive set of over 150 features is created from the raw order book data. These features are designed to capture a wide range of market microstructure signals, including:
    * **Price Dynamics:** Returns, momentum, and rolling volatilities.
    * **Order Book Metrics:** Spreads, relative spreads, and volume-weighted average prices (VWAP).
    * **Liquidity & Imbalance:** Total volume, volume imbalance, and their rolling statistics.
    * **Technical Indicators:** Features derived from RSI and Bollinger Bands.
    * **Lagged & Interaction Features:** Past values and products of key variables to capture temporal dependencies and non-linear relationships.
3.  **Target Transformation:** Different mathematical transformations of the target IV (`label`) are evaluated. The pipeline selects the transformation that yields the highest validation correlation. The `smooth_5` and `sqrt` transforms were found to be particularly effective in initial runs.
4.  **Time-Series Validation:** A `TimeSeriesSplit` with 3 folds is used to ensure the model is trained only on historical data and evaluated on a future-looking validation set, preventing data leakage.
5.  **Ensemble Modeling:** An `EnhancedEnsemble` class trains multiple diverse models (LightGBM with different hyperparameters and CatBoost) on the best-performing target transform. The final prediction is a weighted average of the individual model predictions, with weights determined by each model's validation correlation.
6.  **Prediction and Submission:** The trained ensemble is used to generate predictions for the test dataset. Predictions are clipped to the 1st and 99th percentiles of the training target to prevent extreme outliers. The final predictions are saved to a `submission.csv` file in the required format.

## Results

The pipeline consistently achieves a high Pearson Correlation Score on the validation set, demonstrating its effectiveness. The summary of a typical run is as follows:

* **Total Features Used:** 150
* **Selected Target Transform:** `sqrt`
* **Best Single Model Validation Correlation:** 0.6765
* **Final Ensemble Validation Correlation:** **0.7005**

The visualizations generated by the notebook provide a clear picture of the model's performance, showing how well it tracks the true IV values and maintains a similar distribution.

## Requirements

The pipeline can be run directly within a Kaggle Notebook. The necessary libraries are automatically installed if not already present.

* `pandas`
* `numpy`
* `matplotlib`
* `lightgbm`
* `catboost` (optional, but highly recommended)
* `scikit-learn`
* `scipy`

## How to Run

1.  Open this notebook on Kaggle.
2.  Ensure the necessary datasets are linked via the competition page.
3.  Run all cells. The script will automatically perform data loading, feature engineering, model training, and generate the `submission.csv` file.

##  Author

Manvendra Saini developed this solution. If you have any questions, please contact manvendrasaini2005@gmail.com.
